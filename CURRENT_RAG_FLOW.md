# Current RAG Flow - Complete Overview

## ğŸ—ï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Endpoint                              â”‚
â”‚              POST /agent/chat                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: INPUT GUARDRAIL                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  â€¢ check_input_safety()                                         â”‚
â”‚  â€¢ Uses: gpt-4o-mini (temperature=0)                            â”‚
â”‚  â€¢ Checks: Prompt injection, harmful content, bypass attempts   â”‚
â”‚  â€¢ Result: ALLOW or BLOCK                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ (if ALLOW)
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: DOCUMENT AGENT (LangGraph)                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Node 1: RETRIEVE NODE (async)                            â”‚ â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  A) Vector Search                                        â”‚ â”‚
â”‚  â”‚     â€¢ retrieve_tool(query, k=10)                         â”‚ â”‚
â”‚  â”‚     â€¢ Uses: ChromaDB vector store                        â”‚ â”‚
â”‚  â”‚     â€¢ Embedding: sentence-transformers/all-MiniLM-L6-v2  â”‚ â”‚
â”‚  â”‚     â€¢ Retrieves: Top 10 documents (increased for rerank) â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  B) Reranking (if docs found)                            â”‚ â”‚
â”‚  â”‚     â€¢ rerank(question, docs, top_k=3)                    â”‚ â”‚
â”‚  â”‚     â€¢ Uses: gpt-4o-mini (temperature=0)                  â”‚ â”‚
â”‚  â”‚     â€¢ Process:                                            â”‚ â”‚
â”‚  â”‚       1. Score each doc chunk (0-1 relevance)            â”‚ â”‚
â”‚  â”‚       2. Parallel async scoring (asyncio.gather)        â”‚ â”‚
â”‚  â”‚       3. Sort by score (descending)                      â”‚ â”‚
â”‚  â”‚       4. Return top 3 chunks                             â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  Output: state["context"] = [top 3 ranked chunks]        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Node 2: GENERATE NODE                                     â”‚ â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  â€¢ Gets: context (top 3 chunks) + conversation history    â”‚ â”‚
â”‚  â”‚  â€¢ Uses: gpt-4o (temperature=0.1)                         â”‚ â”‚
â”‚  â”‚  â€¢ Prompt:                                                â”‚ â”‚
â”‚  â”‚    - Context: Top 3 reranked documents                    â”‚ â”‚
â”‚  â”‚    - History: Previous conversation turns (max 6)         â”‚ â”‚
â”‚  â”‚    - Question: User's question                             â”‚ â”‚
â”‚  â”‚    - Rules: Strict RAG, respond "I don't know" if no     â”‚ â”‚
â”‚  â”‚             answer in context                              â”‚ â”‚
â”‚  â”‚  â€¢ Saves: Conversation to Memory                          â”‚ â”‚
â”‚  â”‚  â€¢ Output: state["answer"] = LLM response                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: OUTPUT GUARDRAIL                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  â€¢ check_output_safety(answer)                                 â”‚
â”‚  â€¢ Uses: gpt-4o-mini (temperature=0)                           â”‚
â”‚  â€¢ Checks: Unsafe/harmful content in response                  â”‚
â”‚  â€¢ Result: ALLOW (return as-is) or REDACT (sanitize)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Final Response                                â”‚
â”‚  {                                                              â”‚
â”‚    "answer": "...",                                            â”‚
â”‚    "guardrail": {                                              â”‚
â”‚      "stage": "none" | "output",                               â”‚
â”‚      "blocked": false | true,                                  â”‚
â”‚      "reason": null | "..."                                    â”‚
â”‚    }                                                            â”‚
â”‚  }                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Complete Flow Breakdown

### **Entry Point: `/agent/chat` API**

**File**: `app/routers/agent.py`

**Input**:
```json
{
  "session_id": "user-session-123",
  "question": "How does circuit breaker protect A1?",
  "reset_session": false
}
```

---

### **Step 1: Input Guardrail** ğŸ”’

**File**: `app/agents/guardrails.py`  
**Function**: `check_input_safety()`

**What it does**:
- Uses `gpt-4o-mini` (temperature=0) as a security classifier
- Checks for:
  - Prompt injection attempts
  - System instruction bypasses
  - Harmful/illegal content requests
  - Guardrail override attempts

**Decision**:
- `ALLOW` â†’ Continue to RAG agent
- `BLOCK` â†’ Return error, stop processing

**Traced**: âœ… Yes (`@traceable`)

---

### **Step 2: Document Agent (LangGraph)** ğŸ¤–

**File**: `app/agents/doc_agent.py`  
**Function**: `run_document_agent()`

**Graph Structure**:
```
START â†’ retrieve_node â†’ generate_node â†’ END
```

#### **Node 1: Retrieve Node** (`retrieve_node`)

**A. Vector Search** (`retrieve_tool`)
- **File**: `app/agents/tools.py`
- **Vector DB**: ChromaDB (persisted in `./ragdb`)
- **Embeddings**: HuggingFace `sentence-transformers/all-MiniLM-L6-v2`
- **Retrieval**: Top **10 documents** (k=10, increased for reranking)
- **Method**: Cosine similarity search on embeddings

**B. Reranking** (`rerank`)
- **File**: `app/agents/reranker.py`
- **Model**: `gpt-4o-mini` (temperature=0)
- **Process**:
  1. For each of 10 retrieved docs:
     - Send to LLM: "Score relevance 0-1 for this question"
     - Get numeric score
     - Clamp to [0, 1] range
  2. **Parallel Processing**: All 10 docs scored simultaneously (`asyncio.gather`)
  3. Sort by score (highest first)
  4. Return **top 3** chunks

**Why Two-Stage Retrieval?**
- **Vector search**: Fast, semantic similarity (catches related concepts)
- **Reranking**: Precise relevance scoring (LLM understands context better)
- **Result**: Best of both worlds - fast + accurate

**Output**: `state["context"] = [top 3 ranked document chunks]`

**Traced**: âœ… Yes (both `retrieve_tool` and `rerank` have `@traceable`)

---

#### **Node 2: Generate Node** (`generate_node`)

**What it does**:
1. **Gets Context**:
   - Top 3 reranked documents (from retrieve_node)
   - Conversation history (from Memory, max 6 turns)

2. **Builds Prompt**:
   ```
   You are a strict RAG assistant.
   Use ONLY the provided context to answer.
   
   Context: [top 3 chunks]
   History: [previous conversation]
   Question: [user question]
   
   RULES:
   - If answer not in context, respond "I don't know based on the documents."
   ```

3. **Calls LLM**:
   - Model: `gpt-4o` (temperature=0.1)
   - Low temperature = consistent, factual responses

4. **Saves to Memory**:
   - Stores (question, answer) pair for conversation history

**Output**: `state["answer"] = LLM response`

**Traced**: âœ… Yes (`@traceable`)

---

### **Step 3: Output Guardrail** ğŸ”’

**File**: `app/agents/guardrails.py`  
**Function**: `check_output_safety()`

**What it does**:
- Uses `gpt-4o-mini` (temperature=0) as safety classifier
- Checks if answer contains:
  - Unsafe/harmful content
  - Disallowed information
  - Inappropriate responses

**Decision**:
- `ALLOW` â†’ Return answer as-is
- `REDACT` â†’ Replace with safe message: "I'm not able to answer that safely..."

**Traced**: âœ… Yes (`@traceable`)

---

## ğŸ”§ Key Components

### **1. Vector Database**
- **Type**: ChromaDB
- **Location**: `./ragdb/` (persisted locally)
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2`
- **Chunk Size**: 800 characters, 200 overlap

### **2. LLM Models Used**

| Component | Model | Temperature | Purpose |
|-----------|-------|-------------|---------|
| Main Answer | gpt-4o | 0.1 | Generate final answer |
| Reranker | gpt-4o-mini | 0 | Score document relevance |
| Guardrails | gpt-4o-mini | 0 | Safety classification |
| Tools | gpt-4o-mini | 0.1 | Summarization (if used) |

### **3. Memory System**
- **Type**: In-memory session-based
- **Storage**: `{session_id: [(user_msg, assistant_msg), ...]}`
- **History**: Last 6 turns per session
- **File**: `app/models/memory.py`

### **4. Observability**
- **LangSmith**: All components traced
- **Logging**: Structured logging at each step
- **Metrics**: Latency, tokens, costs tracked

---

## ğŸ”„ Complete Request Flow

```
1. User sends: POST /agent/chat
   â†“
2. Input Guardrail checks safety
   â†“ (if ALLOW)
3. Document Agent starts:
   â”œâ”€â†’ Vector Search (top 10)
   â”œâ”€â†’ Rerank (top 3, parallel scoring)
   â””â”€â†’ Generate answer (gpt-4o)
   â†“
4. Output Guardrail checks safety
   â†“
5. Return response with guardrail status
```

---

## ğŸ“Š Performance Characteristics

### **Retrieval**
- **Vector Search**: ~50-100ms (depends on DB size)
- **Reranking**: ~500-2000ms (10 parallel LLM calls)
- **Total Retrieval**: ~600-2100ms

### **Generation**
- **LLM Call**: ~1000-3000ms (gpt-4o)
- **Total Generation**: ~1000-3000ms

### **Total Pipeline**
- **Best Case**: ~1.6 seconds
- **Average**: ~3-4 seconds
- **Worst Case**: ~5-6 seconds

---

## ğŸ¯ Key Features

âœ… **Two-Stage Retrieval**: Vector search + LLM reranking  
âœ… **Parallel Reranking**: All docs scored simultaneously  
âœ… **Conversation Memory**: Session-based history  
âœ… **Input/Output Guardrails**: Safety at both ends  
âœ… **Strict RAG**: Only answers from context  
âœ… **LangSmith Tracing**: Full observability  
âœ… **Error Handling**: Graceful degradation  
âœ… **Async Support**: Non-blocking operations  

---

## ğŸ” What Makes This RAG Flow Effective

1. **Two-Stage Retrieval**:
   - Vector search finds semantically similar docs (fast)
   - LLM reranking scores true relevance (accurate)
   - Result: Best of both approaches

2. **Parallel Processing**:
   - Reranking scores all docs simultaneously
   - Reduces latency significantly

3. **Strict RAG Prompting**:
   - Forces LLM to only use provided context
   - Prevents hallucination
   - Returns "I don't know" when context insufficient

4. **Safety Layers**:
   - Input guardrail: Prevents malicious queries
   - Output guardrail: Prevents unsafe responses
   - Defense in depth

5. **Observability**:
   - Every step traced in LangSmith
   - Easy to debug and optimize
   - Performance metrics available

---

## ğŸ“ Files Involved

- `app/routers/agent.py` - API endpoint
- `app/agents/doc_agent.py` - LangGraph agent
- `app/agents/reranker.py` - Document reranking
- `app/agents/tools.py` - Retrieval tools
- `app/agents/guardrails.py` - Safety checks
- `app/models/embeddings.py` - Vector DB setup
- `app/models/memory.py` - Conversation history
- `app/config.py` - LangSmith configuration

---

This is your complete RAG pipeline! ğŸš€

